{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5253c8cb-8958-45b1-848e-b8115826a4aa",
   "metadata": {},
   "source": [
    "# Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadcfdb0-9630-493a-bd3e-623933ef20a9",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of a value for a particular variable or feature. It can happen for various reasons, such as data corruption, data loss during transfer, or a user not providing data for some attributes.\n",
    "\n",
    "It is essential to handle missing values because they can cause errors in the data analysis process, leading to incorrect conclusions and predictions. Moreover, some machine learning algorithms cannot handle missing values, so they need to be removed or imputed before applying those algorithms.\n",
    "\n",
    "Some algorithms that are not affected by missing values are:\n",
    "\n",
    "* Decision Trees\n",
    "* Random Forests\n",
    "* Gradient Boosting Machines (GBM)\n",
    "* AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc547be-b15e-463d-9ae9-f12d09e7991f",
   "metadata": {},
   "source": [
    "# Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483957ab-d5ca-41cb-9057-98f0a0ac9b4f",
   "metadata": {},
   "source": [
    "There are several techniques to handle missing data in a dataset. Here are a few of them with examples using Python code:\n",
    "\n",
    "### Deletion:\n",
    "\n",
    "* a. Listwise Deletion: In this technique, the entire row is deleted if it contains even a single missing value. This technique is used when the number of missing values is small and spread randomly.\n",
    "\n",
    "* b. Pairwise Deletion: In this technique, only the missing values are removed from the data, and the remaining values are used for analysis. This technique is used when missing values are random and the number of missing values is high.\n",
    "\n",
    "### Imputation:\n",
    "\n",
    "* a. Mean Imputation: In this technique, the missing values are replaced by the mean of the non-missing values in the column. This technique is used when the missing values are less than 5% of the total data and the data is not skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d2a8b-e4ab-4a48-95a0-a766c53f0912",
   "metadata": {},
   "source": [
    "# Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad09f92-4743-4d82-931b-d1f984afea28",
   "metadata": {},
   "source": [
    "Imbalanced data is a situation where the proportion of observations in different classes in a dataset is not equal. For instance, in a binary classification problem, if the positive class comprises only 10% of the data and the negative class comprises 90% of the data, then it is an imbalanced dataset. Imbalanced data is common in real-world scenarios, such as fraud detection, medical diagnosis, and anomaly detection.\n",
    "\n",
    "If imbalanced data is not handled properly, then the performance of machine learning models can be severely impacted. Models trained on imbalanced data tend to be biased towards the majority class, leading to poor performance on the minority class. For example, a model trained on imbalanced data may predict all observations as the majority class and achieve high accuracy, but it will fail to predict the minority class.\n",
    "\n",
    "Handling imbalanced data is essential to achieve good performance on both classes in a binary classification problem. Some common techniques to handle imbalanced data are:\n",
    "\n",
    "* Oversampling: Oversampling is a technique where the minority class is artificially increased by replicating samples from the minority class. One common oversampling technique is the Synthetic Minority Over-sampling Technique (SMOTE), which generates synthetic samples based on the existing minority samples.\n",
    "\n",
    "* Undersampling: Undersampling is a technique where the majority class is reduced by randomly removing samples from the majority class. One common undersampling technique is the Random Under-sampling.\n",
    "\n",
    "* Class weight balancing: This is a technique where the class weights are adjusted to give more weight to the minority class. This can be done by setting the class_weight parameter in the model.\n",
    "\n",
    "* Ensemble techniques: Ensemble techniques, such as bagging, boosting, and stacking, can be used to combine multiple models trained on different subsamples of the data to achieve better performance on imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3bc76-90de-4197-9041-d83026b7df4a",
   "metadata": {},
   "source": [
    "# Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaba7db-3e89-4530-8bf0-bf5490f279cf",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two techniques used in handling imbalanced datasets.\n",
    "\n",
    "Up-sampling is the process of increasing the number of instances in the minority class by randomly replicating them. This is done until the number of instances in the minority class is equal to the number of instances in the majority class. For example, in a dataset with 100 instances of the majority class and 20 instances of the minority class, up-sampling involves creating additional 80 instances of the minority class, resulting in a balanced dataset.\n",
    "\n",
    "Down-sampling, on the other hand, is the process of reducing the number of instances in the majority class by randomly removing instances. This is done until the number of instances in the majority class is equal to the number of instances in the minority class.\n",
    "\n",
    "Up-sampling is typically used when the minority class is under-represented and the available data is limited, while down-sampling is used when the majority class is over-represented and there is sufficient data available.\n",
    "\n",
    "For example, consider a dataset with two classes: 'fraudulent' and 'non-fraudulent' transactions. The 'fraudulent' class is the minority class with only 5% of the data while the 'non-fraudulent' class comprises 95% of the data. In this case, up-sampling the 'fraudulent' class will be necessary to balance the dataset, as replicating the minority class can provide more data to train the model and improve its performance. However, if the 'non-fraudulent' class had an over-representation of data, down-sampling may be used to create a balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eafae4-b684-4a5f-ba14-106bd8f06001",
   "metadata": {},
   "source": [
    "# Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed48fa8-811b-46b7-952c-dbb2ee6de8fe",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used in machine learning and deep learning to increase the size of the training data by generating new data from the existing data. It involves applying a series of transformations to the existing data, such as flipping, rotating, scaling, adding noise, etc., to create new samples that are similar to the original data. Data augmentation can help in reducing overfitting and improving the performance of the model.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a type of data augmentation technique used to handle imbalanced datasets. It works by creating synthetic samples of the minority class by interpolating between the existing samples of the minority class. SMOTE selects a random sample from the minority class and finds its k nearest neighbors. It then generates new samples by randomly selecting points along the line segments connecting the selected sample and its k nearest neighbors. The resulting synthetic samples are then added to the original dataset.\n",
    "\n",
    "For example, suppose you have a dataset with two classes, A and B, and class A has only 10% of the data. To balance the dataset, you can use SMOTE to generate synthetic samples of class A, so that it has the same number of samples as class B. SMOTE will create new samples of class A by interpolating between the existing samples of class A, resulting in a balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e9804-5ed5-4c97-a98d-26bd72ba8ff3",
   "metadata": {},
   "source": [
    "# Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4911a-e1ad-4ece-bf61-08840a52e0a5",
   "metadata": {},
   "source": [
    "Outliers in a dataset refer to observations or data points that are significantly different from other observations in the same dataset. Outliers are data points that lie far away from the central tendency of a distribution and can have a significant effect on the overall analysis of the dataset.\n",
    "\n",
    "It is essential to handle outliers because they can affect the statistical analysis of the dataset, leading to inaccurate and unreliable results. Outliers can affect the measures of central tendency, such as the mean and median, and can also affect measures of variability, such as the standard deviation and variance.\n",
    "\n",
    "Handling outliers depends on the type of analysis being performed. Sometimes outliers are genuinely part of the dataset and should not be removed, while other times they need to be treated or removed to obtain accurate results.\n",
    "\n",
    "There are several techniques to handle outliers, such as:\n",
    "\n",
    "* Z-score: It is a statistical technique that involves calculating the Z-score of each data point and removing the points that have a Z-score greater than a certain threshold value.\n",
    "\n",
    "* Winsorizing: It involves replacing the extreme values of the dataset with less extreme values, typically the upper or lower limit of the dataset.\n",
    "\n",
    "* Interquartile range (IQR) method: It involves identifying the lower and upper quartile of the dataset and calculating the IQR. Data points that lie outside the lower and upper quartile by a certain factor (e.g., 1.5 times the IQR) are considered outliers and can be removed or treated.\n",
    "\n",
    "* Tukey's method: It involves using a similar approach to the IQR method, but with a different threshold value based on Tukey's fences.\n",
    "\n",
    "Handling outliers is important to ensure that the statistical analysis of the dataset is accurate and reliable, and to avoid drawing incorrect conclusions based on misleading data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3878a-421f-4b0c-a97c-6db1690123ac",
   "metadata": {},
   "source": [
    "# Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3714ac8-2496-4299-9a2a-f861bd320176",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis. Some of them are:\n",
    "\n",
    "* Deletion: If the amount of missing data is small, we can delete the rows or columns that contain missing data. However, this technique is not recommended as it can lead to loss of valuable information.\n",
    "\n",
    "* Imputation: In this technique, we replace missing data with estimated values. The most commonly used methods for imputation are mean, median, mode, and regression imputation.\n",
    " \n",
    "* Hot-deck imputation: In this technique, we replace missing values with values from similar records.\n",
    "\n",
    "* Multiple imputations: In this technique, we create multiple imputations of the missing data using various imputation techniques and combine them to obtain a final estimate.\n",
    "\n",
    "* Predictive modeling: In this technique, we use predictive models to estimate missing data based on other variables.\n",
    "\n",
    "The choice of technique depends on the amount of missing data, the type of data, and the specific requirements of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03c590-8b30-4237-b8be-04b7c00a1666",
   "metadata": {},
   "source": [
    "# Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e5c08-b3d9-49c2-bdf3-db972c3d6afb",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data. Some of these strategies include:\n",
    "\n",
    "* Visual inspection: One way to determine if there is a pattern to the missing data is to plot the data and look for any obvious patterns or correlations between missing values.\n",
    "\n",
    "* Correlation analysis: Another way to determine if there is a pattern to the missing data is to calculate the correlation between missing values and other variables in the dataset. If there is a high correlation, then there may be a pattern to the missing data.\n",
    "\n",
    "* Imputation: Imputation is a statistical technique used to fill in missing values with estimated values. If the imputed values are similar to the observed values, then the missing data is likely missing at random. However, if the imputed values are different from the observed values, then there may be a pattern to the missing data.\n",
    "\n",
    "* Hypothesis testing: Hypothesis testing can be used to determine if there is a significant difference between the missing values and the observed values. If there is no significant difference, then the missing data is likely missing at random. However, if there is a significant difference, then there may be a pattern to the missing data.\n",
    "\n",
    "* Machine learning algorithms: Machine learning algorithms can be used to predict missing values based on other variables in the dataset. If the accuracy of the predictions is high, then the missing data is likely missing at random. However, if the accuracy of the predictions is low, then there may be a pattern to the missing data.\n",
    "\n",
    "Overall, it is important to use a combination of these strategies to determine if the missing data is missing at random or if there is a pattern to the missing data. This will help ensure that any analyses or models built using the data are accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd032726-a144-4d5b-9f5c-1766a9745e83",
   "metadata": {},
   "source": [
    "# Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18a2a6-4d11-49cb-94c0-48e096f5da98",
   "metadata": {},
   "source": [
    "There are several strategies you can use to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "* Confusion matrix: Use a confusion matrix to evaluate the performance of your model. It provides information about true positive, true negative, false positive, and false negative rates. It helps to identify if the model is biased towards one class.\n",
    "\n",
    "* ROC curve: Receiver Operating Characteristic (ROC) curve is a plot between True Positive Rate and False Positive Rate. It is a better metric to evaluate performance on imbalanced datasets, as it does not consider the threshold value.\n",
    "\n",
    "* Precision-Recall Curve: The precision-recall curve is a plot of precision versus recall. This is also a better metric to evaluate performance on imbalanced datasets. It provides a trade-off between precision and recall.\n",
    "\n",
    "* F1 Score: F1 Score is the harmonic mean of precision and recall. It is a good metric to evaluate the overall performance of the model.\n",
    "\n",
    "* Resampling: Resampling techniques like up-sampling, down-sampling, and SMOTE can be used to balance the dataset. Once the dataset is balanced, you can use standard evaluation metrics like accuracy, precision, recall, and F1 score.\n",
    "\n",
    "* Cross-validation: Use cross-validation techniques like stratified k-fold cross-validation to ensure that each fold has a balanced representation of the classes. This will give you an estimate of the performance of the model on unseen data.\n",
    "\n",
    "Cost-sensitive learning: In some cases, you may need to assign a higher cost to misclassification of the minority class. In such cases, cost-sensitive learning techniques can be used to improve the performance of the model on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e57b2-f3b2-4af7-aca6-367067c721b4",
   "metadata": {},
   "source": [
    "# Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f308974-da6e-48a1-a9c2-57fa6762a7d5",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset, the following methods can be employed:\n",
    "\n",
    "* Undersampling: randomly removing examples from the majority class until the dataset is balanced.\n",
    "\n",
    "* Oversampling: replicating examples from the minority class until the dataset is balanced.\n",
    "\n",
    "* Synthetic Minority Oversampling Technique (SMOTE): generates synthetic examples of the minority class to create a balanced dataset.\n",
    "\n",
    "=> To down-sample the majority class, undersampling can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b49dd7-3375-4911-8a43-44cc7ab96f45",
   "metadata": {},
   "source": [
    "# Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec620252-fe65-4fb3-96e1-8eb83dcef8b6",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset, where the occurrence of a rare event is very low, up-sampling the minority class can be a useful technique to balance the dataset. Here are some methods that can be employed to up-sample the minority class:\n",
    "\n",
    "* Random over-sampling: This method involves randomly duplicating instances from the minority class until it is balanced with the majority class. It is a simple and effective method, but it may lead to overfitting.\n",
    "\n",
    "* Synthetic Minority Over-sampling Technique (SMOTE): This method involves creating synthetic instances of the minority class by interpolating between neighboring instances. It is a widely used technique for up-sampling and can produce high-quality synthetic samples.\n",
    "\n",
    "* Adaptive Synthetic Sampling (ADASYN): This method is an extension of SMOTE, which generates more synthetic examples for the minority class that are difficult to learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
